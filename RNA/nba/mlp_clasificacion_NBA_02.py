# -*- coding: utf-8 -*-
"""Copia de MLP-ClasificacionNBA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wPf3QBJssNaHrL1owYPw8_mp06vJY0Cs

Integrantes: Carcelen Jorge , Pulamarin Brayan , Ruiz Mauricio

**Importar Módulos**
"""

import matplotlib.pyplot as plt
from sklearn import preprocessing 
import numpy as np
from keras.models import Sequential
from keras.layers.core import Dense
import pandas as pd
from sklearn.metrics import confusion_matrix
import seaborn as sns
from sklearn.metrics import accuracy_score
from sklearn.metrics import recall_score
from sklearn.metrics import precision_score
from sklearn.metrics import f1_score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import r2_score
from sklearn.metrics import roc_curve
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
import tensorflow as tf

"""**Cargar los Datos**"""

dataset= pd.read_excel('nba_data.xlsx')
datos = dataset.values
datos.shape

# from sklearn.impute import SimpleImputer
imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
imputer.fit(datos[:, 0:19])
datos[:, 0:19] = imputer.transform(datos[:, 0:19])

"""**Técnicas de Escalamiento, Normalización y Estandarización de Datos con Scikit-Learn**

**MinMaxScaler**

-Se usa para realizar el Escalamiento.

-Utiliza un rango que se calcula como la diferencia entre el máximo y el mínimo .

-Para cada valor se sustrae el mínimo de ese valor y luego se divide entre el rango.

-Conserva la forma de la distribución original

-Se conserva a los datos atípicos

-Tiene un rango por defecto de 0 a 1.

-La media varia.

-Se puede usar como primera opción.
"""

datos_min_max = preprocessing.MinMaxScaler().fit_transform(datos)

datos_min_max

"""**Normalizer**

Hay que tomar en cuenta que trabaja en las filas y no en las columnas.

-Transforma los valores entre -1 y 1.

-Raramente se usa.

"""

datos_normalizer = preprocessing.Normalizer().transform(datos.T)
datos_normalizer = datos_normalizer.T
datos_normalizer

"""**RobustScaler**

**Estandarización **

-A la observación se le sustrae la media y se divide entre el rango intercuartil.

-No escala a un rango predeterminado

-El rango varia y es más grande que el MinMaxScaler

-La media varia

-Se usa cuando tenemos datos atípicos y no se desea que estos tengan mucha influencia.

"""

datos_robust_scaler = preprocessing.RobustScaler().fit_transform(datos)
datos_robust_scaler

"""**StandardScaler**

**Estandarización **

Sustrae la media de la observación y luego escala a la unidad de varianza es decir la desviación estándar.

•	El resultado es una distribución con una desviación estándar de 1 , la varianza también será de 1.

•	El rango varia.

•	La media es 0.

•	Se usa cuando necesitamos que una característica sea cercana a una distribución normal

•	Si existen datos atípicos estos se escalarán a un intervalo más pequeño.

"""

datos_standard_scaler = preprocessing.StandardScaler().fit_transform(datos)
datos_standard_scaler

"""**Comparación de métodos**"""

# crea una figura con 5 subfiguras para comparar los métodos
fig = plt.figure(figsize=(20, 5))
ax1 = fig.add_subplot(1, 5, 1)
ax2 = fig.add_subplot(1, 5, 2)
ax3 = fig.add_subplot(1, 5, 3)
ax4 = fig.add_subplot(1, 5, 4)
ax5 = fig.add_subplot(1, 5, 5)

# crea y personaliza series de datos
ax1.set_title("Datos")
ax1.plot(datos, linewidth=0, marker="*", color="red", markersize=4)

ax2.set_title("Min Max")
ax2.plot(datos_min_max, linewidth=0, marker="*", color="blue", markersize=4)

ax3.set_title("Normalizer")
ax3.plot(datos_normalizer, linewidth=0, marker="*", color="green", markersize=4)
#ax3.set_ylim(0, 1)

ax4.set_title("Standard Scaler")
ax4.plot(datos_standard_scaler, linewidth=0, marker="*", color="orange", markersize=4)

ax5.set_title("Robust Scaler")
ax5.plot(datos_robust_scaler, linewidth=0, marker="*", color="black", markersize=4)
plt.savefig("comparacion-metodos")
plt.show()

"""**Dividir los datos para training set y test set**"""

entrada = datos_min_max[:,0:19] 
salida = datos_min_max[:,19]
# cargamos los datos
training_data = np.array(entrada, "float32")
# ground truth
target_data = np.array(salida, "float32")

trainX, testX, trainy, testy = train_test_split(training_data, target_data ,test_size=0.3)

"""**Establecer los parametros del modelo**"""

opt = tf.keras.optimizers.SGD(learning_rate=0.001 , momentum=0.9)
model = Sequential()
model.add(Dense(15, activation='relu', input_dim=19))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy',
              optimizer=opt,
              metrics=['accuracy'])
 
data_modelo = model.fit(trainX, trainy, validation_data=(testX,testy),epochs=10 ,batch_size=4)

"""**Evaluar el modelo**"""

# evaluamos el modelo
print("Evaluación del modelo test")
scores_test = model.evaluate(testX, testy)
print("\%s: %.4f" % (model.metrics_names[0], scores_test[0])) # loss
print("\n%s: %.2f%%" % (model.metrics_names[1], scores_test[1]*100)) # accuracy

print("Evaluación del modelo training")
scores = model.evaluate(trainX, trainy)
print("\%s: %.4f" % (model.metrics_names[0], scores[0])) # loss
print("\n%s: %.2f%%" % (model.metrics_names[1], scores[1]*100)) # accuracy

# full metrics
y_pred = model.predict(testX).round()
y_true = testy.round()

"""**Matriz de confusión**"""
print(y_pred)
print(y_true)